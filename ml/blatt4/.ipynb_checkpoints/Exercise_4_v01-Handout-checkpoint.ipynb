{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "188626d4",
   "metadata": {},
   "source": [
    "# Exercise sheet 4"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba9292d9",
   "metadata": {},
   "source": [
    "In this exercise sheet we will code all components needed to build and train a (deep) neural network. There will be multiple functions to implement, most of which are followed by a verification block that allows you to check if your function works as expected.\n",
    "\n",
    "**A note on the used notation**:\n",
    "\n",
    "- Superscript $[l]$ denotes a quantity associated with the $l^{th}$ layer. \n",
    "    - Example: $a^{[L]}$ is the $L^{th}$ layer activation. $W^{[L]}$ and $b^{[L]}$ are the $L^{th}$ layer parameters.\n",
    "- Superscript $(i)$ denotes a quantity associated with the $i^{th}$ example. \n",
    "    - Example: $x^{(i)}$ is the $i^{th}$ training example.\n",
    "- Lowerscript $i$ denotes the $i^{th}$ entry of a vector.\n",
    "    - Example: $a^{[l]}_i$ denotes the $i^{th}$ entry of the $l^{th}$ layer's activations).\n",
    "\n",
    "\n",
    "We start with loading some standard libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e8c3ba27",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plt to plot figures\n",
    "import matplotlib.pyplot as plt\n",
    "# numpy for array/matrix operations\n",
    "import numpy as np\n",
    "# to be able to load matlab data files\n",
    "from scipy.io import loadmat\n",
    "\n",
    "# we set a seed variable for functions that use randomization (e.g. when shuffling data samples)\n",
    "# this way, we can have reproducible results even with randomization\n",
    "RANDOM_STATE = 2\n",
    "np.random.seed(RANDOM_STATE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "441ff499",
   "metadata": {},
   "source": [
    "## 1) Outline\n",
    "\n",
    "To build your neural network, you will be implementing several \"helper functions\". These helper functions can be used to build a L-layer neural network. Each small helper function you will implement will have instructions that will walk you through the necessary steps. Here is an outline of this assignment, you will:\n",
    "\n",
    "- Initialize the parameters for an L-layer neural network.\n",
    "- Implement the forward propagation module (shown in purple in the figure below).\n",
    "    - Complete the LINEAR part of a layer's forward propagation step (resulting in output Z).\n",
    "    - Implement activation functions (relu/sigmoid)\n",
    "    - Combine the previous two steps into a new [LINEAR->ACTIVATION] forward function.\n",
    "    - Stack the [LINEAR->RELU] forward function L-1 time (for layers 1 through L-1) and add a [LINEAR->SIGMOID] at the end (for the final layer L). This gives you a new L_model_forward function.\n",
    "- Compute the loss.\n",
    "- Implement the backward propagation module (denoted in red in the figure below).\n",
    "\n",
    "    - Complete the LINEAR part of a layer's backward propagation step.\n",
    "    - Functions for calculating the gradient of the ACTIVATION functions are given (relu_backward/sigmoid_backward)\n",
    "    - Combine the previous two steps into a new [LINEAR->ACTIVATION] backward function.\n",
    "    - Stack [LINEAR->RELU] backward L-1 times and add [LINEAR->SIGMOID] backward in a new L_model_backward function\n",
    "\n",
    "- Finally update the parameters using gradient descent.\n",
    "\n",
    "Note that for every forward function, there is a corresponding backward function. That is why at every step of your forward module you will be storing some values in a cache. The cached values are useful for computing gradients. In the backpropagation module you will then use the cache to calculate the gradients. This assignment will show you exactly how to carry out each of these steps.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6244157",
   "metadata": {},
   "source": [
    "## 2) Initialization of L-layer Neural Network\n",
    "\n",
    "Complete implementing the function below to randomly initialize parameters (weight matrices and bias vectors) for a (deep) L-layer neural network. Make sure that dimensions of the parameter matrices/vectors match between each layer (assert calls for this are already implemented).\n",
    "\n",
    "Notes:\n",
    "- Initialize weight matrices with small normal distributed numbers. Use np.random.randn(matrix_shape) * 0.01.\n",
    "- Initialize bias vectors with zeros. Use np.zeros(vector_shape)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c1a5c5f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_parameters(layer_dims):\n",
    "    \"\"\"\n",
    "    Argument:\n",
    "    layer_dims -- python array (list) containing the dimensions of each layer in our network:\n",
    "                    e.g. a network with input layer (e.g. 4 features), two hidden layers (e.g. each with 5 nodes)\n",
    "                    and output layer (e.g. 1 node for binary classification using sigmoid activation) would result \n",
    "                    in layer_dims=[4,5,5,1]\n",
    "    \n",
    "    Returns:\n",
    "    parameters -- python dictionary containing your parameters \"W1\", \"b1\", ..., \"WL\", \"bL\":\n",
    "                    Wl -- weight matrix of shape (layer_dims[l], layer_dims[l-1])\n",
    "                    bl -- bias vector of shape (layer_dims[l], 1)\n",
    "                    \n",
    "    \"\"\"\n",
    "    np.random.seed(RANDOM_STATE)\n",
    "    parameters = {}\n",
    "    L = len(layer_dims)\n",
    "    \n",
    "    for l in range(1, L):\n",
    "        ### START CODE HERE ### (≈ 2 lines of code)\n",
    "        \n",
    "        # initializing weight matrix 'W' with random values (to introduce diversity in the learning process)\n",
    "        # Without random initialization, all the neurons in a layer would compute the same gradients and update\n",
    "        # their weights in the same way. This limits the capacity of the network to learn diverse features!\n",
    "        parameters['W' + str(l)] = np.random.randn(layer_dims[l], layer_dims[l-1]) * 0.01\n",
    "        \n",
    "        # initializing bias vector 'b' with zeros\n",
    "        parameters['b' + str(l)] = np.zeros((layer_dims[l], 1))\n",
    "        \n",
    "        ### END CODE HERE ###\n",
    "\n",
    "        assert(parameters['W' + str(l)].shape == (layer_dims[l], layer_dims[l-1]))\n",
    "        assert(parameters['b' + str(l)].shape == (layer_dims[l], 1))\n",
    "\n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b368034",
   "metadata": {},
   "source": [
    "**Verify that code works as expected**\n",
    "\n",
    "Compare output of your implemented function with expected output below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ec75f63c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "W1 = [[-4.16757847e-03 -5.62668272e-04 -2.13619610e-02  1.64027081e-02]\n",
      " [-1.79343559e-02 -8.41747366e-03  5.02881417e-03 -1.24528809e-02]\n",
      " [-1.05795222e-02 -9.09007615e-03  5.51454045e-03  2.29220801e-02]\n",
      " [ 4.15393930e-04 -1.11792545e-02  5.39058321e-03 -5.96159700e-03]\n",
      " [-1.91304965e-04  1.17500122e-02 -7.47870949e-03  9.02525097e-05]]\n",
      "b1 = [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "W2 = [[-8.78107893e-03 -1.56434170e-03  2.56570452e-03 -9.88779049e-03\n",
      "  -3.38821966e-03]\n",
      " [-2.36184031e-03 -6.37655012e-03 -1.18761229e-02 -1.42121723e-02\n",
      "  -1.53495196e-03]\n",
      " [-2.69056960e-03  2.23136679e-02 -2.43476758e-02  1.12726505e-03\n",
      "   3.70444537e-03]\n",
      " [ 1.35963386e-02  5.01857207e-03 -8.44213704e-03  9.76147160e-08\n",
      "   5.42352572e-03]\n",
      " [-3.13508197e-03  7.71011738e-03 -1.86809065e-02  1.73118467e-02\n",
      "   1.46767801e-02]]\n",
      "b2 = [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n"
     ]
    }
   ],
   "source": [
    "example_layer_dims = [4,5,5,1]\n",
    "\n",
    "parameters = initialize_parameters(example_layer_dims)\n",
    "print(\"W1 = \" + str(parameters[\"W1\"]))\n",
    "print(\"b1 = \" + str(parameters[\"b1\"]))\n",
    "print(\"W2 = \" + str(parameters[\"W2\"]))\n",
    "print(\"b2 = \" + str(parameters[\"b2\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4462b159",
   "metadata": {},
   "source": [
    "#### Expected output:\n",
    "\n",
    "W1 = [[-4.16757847e-03 -5.62668272e-04 -2.13619610e-02  1.64027081e-02]\n",
    " [-1.79343559e-02 -8.41747366e-03  5.02881417e-03 -1.24528809e-02]\n",
    " [-1.05795222e-02 -9.09007615e-03  5.51454045e-03  2.29220801e-02]\n",
    " [ 4.15393930e-04 -1.11792545e-02  5.39058321e-03 -5.96159700e-03]\n",
    " [-1.91304965e-04  1.17500122e-02 -7.47870949e-03  9.02525097e-05]]\n",
    " \n",
    "b1 = [[0.]\n",
    " [0.]\n",
    " [0.]\n",
    " [0.]\n",
    " [0.]]\n",
    "\n",
    "W2 = [[-8.78107893e-03 -1.56434170e-03  2.56570452e-03 -9.88779049e-03\n",
    "  -3.38821966e-03]\n",
    " [-2.36184031e-03 -6.37655012e-03 -1.18761229e-02 -1.42121723e-02\n",
    "  -1.53495196e-03]\n",
    " [-2.69056960e-03  2.23136679e-02 -2.43476758e-02  1.12726505e-03\n",
    "   3.70444537e-03]\n",
    " [ 1.35963386e-02  5.01857207e-03 -8.44213704e-03  9.76147160e-08\n",
    "   5.42352572e-03]\n",
    " [-3.13508197e-03  7.71011738e-03 -1.86809065e-02  1.73118467e-02\n",
    "   1.46767801e-02]]\n",
    "\n",
    "b2 = [[0.]\n",
    " [0.]\n",
    " [0.]\n",
    " [0.]\n",
    " [0.]]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec0a0e0c",
   "metadata": {},
   "source": [
    "## 3) Forward propagation\n",
    "\n",
    "Now that we have initialized the parameters, we implement the forward propagation module. We start by implementing some basic functions that are needed later on. We'll complete three functions in this part:\n",
    "\n",
    "    - LINEAR\n",
    "    - LINEAR -> ACTIVATION where ACTIVATION will be either ReLU or Sigmoid.\n",
    "    - [LINEAR -> RELU] x (L-1) -> LINEAR -> SIGMOID (i.e. whole model)\n",
    "\n",
    "### 3.1) Linear Forward Function\n",
    "\n",
    "We start with the linear forward function. This function calculates the equation:\n",
    "\n",
    "$$Z^{[l]} = W^{[l]}A^{[l-1]} +b^{[l]}$$\n",
    "\n",
    "with $A^{[0]} = X$ (i.e. the matrix of data samples).\n",
    "\n",
    "Remember that $W^{[l]}A^{[l-1]}$ calculates the dot product between two matrices."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e31e9644",
   "metadata": {},
   "outputs": [],
   "source": [
    "def linear_forward(A, W, b):\n",
    "    \"\"\"\n",
    "    Implement the linear part of a layer's forward propagation.\n",
    "\n",
    "    Arguments:\n",
    "    A -- activations from previous layer (or input data): (size of previous layer, number of examples)\n",
    "    W -- weights matrix: numpy array of shape (size of current layer, size of previous layer)\n",
    "    b -- bias vector, numpy array of shape (size of the current layer, 1)\n",
    "\n",
    "    Returns:\n",
    "    Z -- the input of the activation function, also called pre-activation parameter \n",
    "    cache -- a python tuple containing \"A\", \"W\" and \"b\" ; stored for computing the backward pass efficiently\n",
    "    \"\"\"\n",
    "    \n",
    "    ### START CODE HERE ### (≈ 1 line of code)\n",
    "    Z = np.dot(W, A) + b\n",
    "    ### END CODE HERE ###\n",
    "    \n",
    "    assert(Z.shape == (W.shape[0], A.shape[1]))\n",
    "    cache = (A, W, b)\n",
    "    \n",
    "    return Z, cache"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "626c1bc2",
   "metadata": {},
   "source": [
    "**Verify that code works as expected**\n",
    "\n",
    "Compare output of your implemented function with expected output below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "19e98467",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Z = [[-2.66089417]\n",
      " [ 2.83770075]\n",
      " [-2.7214624 ]\n",
      " [ 0.36664977]\n",
      " [-3.21640702]]\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(RANDOM_STATE)\n",
    "example_batch_size = 1  # number of data samples in this batch\n",
    "\n",
    "A = np.random.randn(example_layer_dims[0], example_batch_size)\n",
    "W = np.random.randn(example_layer_dims[1], example_layer_dims[0])\n",
    "b = np.random.randn(example_layer_dims[1], 1)\n",
    "\n",
    "Z, linear_cache = linear_forward(A, W, b)\n",
    "print(\"Z = \" + str(Z))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f36dfe2",
   "metadata": {},
   "source": [
    "#### Expected output: \n",
    "\n",
    "Z = [[-2.66089417]\n",
    " [ 2.83770075]\n",
    " [-2.7214624 ]\n",
    " [ 0.36664977]\n",
    " [-3.21640702]]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c1b847f",
   "metadata": {},
   "source": [
    "### 3.2) Activation Functions\n",
    "\n",
    "There are various different activation functions. In this exercise, we implement two activation functions that were presented in the lecture: sigmoid and ReLU. We return the input (Z) as well (for simplification of error backpropagation calculations later on).\n",
    "\n",
    "Note: the backward functions for both activation functions (i.e. derivative including application of chain rule - therefore not exactly the same as in the lecture) is already given."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0dab65b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# activation functions\n",
    "\n",
    "def sigmoid(Z):\n",
    "    ### START CODE HERE ### (≈ 1 line of code)\n",
    "    A = 1 / (1 + np.exp(-Z))\n",
    "    ### END CODE HERE ###\n",
    "    return A, Z\n",
    "\n",
    "def relu(Z):\n",
    "    ### START CODE HERE ### (≈ 1 line of code)\n",
    "    A = Z * (Z > 0)\n",
    "    ### END CODE HERE ###\n",
    "    assert(A.shape == Z.shape)\n",
    "    return A, Z\n",
    "\n",
    "\n",
    "# backward functions (includes chain rule application)\n",
    "\n",
    "def sigmoid_backward(dA, Z):    \n",
    "    # version from the lecture:\n",
    "    dZ = dA * np.exp(-Z) / (1 + np.exp(-Z)) ** 2\n",
    "    # alternative version\n",
    "    #s = 1/(1+np.exp(-Z))\n",
    "    #dZ = dA * s * (1 - s)\n",
    "    assert (dZ.shape == Z.shape)\n",
    "    return dZ\n",
    "\n",
    "def relu_backward(dA, Z):\n",
    "    dZ = np.zeros(dA.shape)\n",
    "    dZ[Z > 0] = 1\n",
    "    assert (dZ.shape == Z.shape)\n",
    "    return dZ"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0071fff7",
   "metadata": {},
   "source": [
    "### 3.3) Linear Activation Forward Function\n",
    "\n",
    "One layer in a neural network usually consists of a linear function that is followed by an activation function (and thus the two combined are counted as one layer). For convenience, we group these two operations into one function (LINEAR->ACTIVATION). Implement a function that does the LINEAR forward step followed by an ACTIVATION forward step.\n",
    "\n",
    "The mathematical formulation is given as: \n",
    "$A^{[l]} = g(Z^{[l]}) = g(W^{[l]}A^{[l-1]} +b^{[l]})$ \n",
    "\n",
    "where the activation \"g\" can be sigmoid() or relu(). Use linear_forward() from 3.1) and the correct activation function from 3.2) to implement the function below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c9b1a054",
   "metadata": {},
   "outputs": [],
   "source": [
    "def linear_activation_forward(A_prev, W, b, activation):\n",
    "    \"\"\"\n",
    "    Implement the forward propagation for the LINEAR->ACTIVATION layer\n",
    "\n",
    "    Arguments:\n",
    "    A_prev -- activations from previous layer (or input data): (size of previous layer, number of examples)\n",
    "    W -- weights matrix: numpy array of shape (size of current layer, size of previous layer)\n",
    "    b -- bias vector, numpy array of shape (size of the current layer, 1)\n",
    "    activation -- the activation to be used in this layer, stored as a text string: \"sigmoid\" or \"relu\"\n",
    "\n",
    "    Returns:\n",
    "    A -- the output of the activation function, also called the post-activation value \n",
    "    cache -- a python tuple containing \"linear_cache\" and \"activation_cache\";\n",
    "             stored for computing the backward pass efficiently\n",
    "    \"\"\"\n",
    "    \n",
    "    if activation == \"sigmoid\":\n",
    "        # Inputs: \"A_prev, W, b\". Outputs: \"A, activation_cache\".\n",
    "        ### START CODE HERE ### (≈ 2 lines of code)\n",
    "        Z, linear_cache = linear_forward(A_prev, W, b)\n",
    "        A, activation_cache = sigmoid(Z)\n",
    "        ### END CODE HERE ###\n",
    "    \n",
    "    elif activation == \"relu\":\n",
    "        # Inputs: \"A_prev, W, b\". Outputs: \"A, activation_cache\".\n",
    "        ### START CODE HERE ### (≈ 2 lines of code)\n",
    "        Z, linear_cache = linear_forward(A_prev, W, b)\n",
    "        A, activation_cache = relu(Z)\n",
    "        ### END CODE HERE ###\n",
    "    \n",
    "    assert (A.shape == (W.shape[0], A_prev.shape[1]))\n",
    "    cache = (linear_cache, activation_cache)\n",
    "\n",
    "    return A, cache"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccba2865",
   "metadata": {},
   "source": [
    "**Verify that code works as expected**\n",
    "\n",
    "Compare output of your implemented function with expected output below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0b81fd85",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "With sigmoid: A = [[0.06532072]\n",
      " [0.94467943]\n",
      " [0.06171873]\n",
      " [0.5906492 ]\n",
      " [0.03855294]]\n",
      "With ReLU: A = [[-0.        ]\n",
      " [ 2.83770075]\n",
      " [-0.        ]\n",
      " [ 0.36664977]\n",
      " [-0.        ]]\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(RANDOM_STATE)\n",
    "example_batch_size = 1  # number of data samples in this batch\n",
    "\n",
    "A_prev = np.random.randn(example_layer_dims[0], example_batch_size)\n",
    "W = np.random.randn(example_layer_dims[1], example_layer_dims[0])\n",
    "b = np.random.randn(example_layer_dims[1], 1) \n",
    "\n",
    "A, linear_activation_cache = linear_activation_forward(A_prev, W, b, activation = \"sigmoid\")\n",
    "print(\"With sigmoid: A = \" + str(A))\n",
    "\n",
    "A, linear_activation_cache = linear_activation_forward(A_prev, W, b, activation = \"relu\")\n",
    "print(\"With ReLU: A = \" + str(A))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4985d8ed",
   "metadata": {},
   "source": [
    "#### Expected output:\n",
    "    \n",
    "With sigmoid: A = [[0.06532072]\n",
    " [0.94467943]\n",
    " [0.06171873]\n",
    " [0.5906492 ]\n",
    " [0.03855294]]\n",
    " \n",
    "With ReLU: A = [[0.        ]\n",
    " [2.83770075]\n",
    " [0.        ]\n",
    " [0.36664977]\n",
    " [0.        ]]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "454db058",
   "metadata": {},
   "source": [
    "### 3.4) L-Layer Model\n",
    "\n",
    "We can now use the implemented functions to build an L-layer neural network with a sigmoid activation function in the output layer. Write a function that creates a neural network with L-1 layers using linear_activation_forward with ReLU activation, then finishes with the output layer, using linear_activation_forward with sigmoid activation.\n",
    "\n",
    "Note on counting layers: when counting layers in a neural network, we only count layers with learnable parameters (weight matrices / bias vectors). This is why the input layer is usually NOT counted."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "989b6cdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def L_model_forward(X, parameters):\n",
    "    \"\"\"\n",
    "    Implement forward propagation for the [LINEAR->RELU]*(L-1)->LINEAR->SIGMOID computation\n",
    "    \n",
    "    Arguments:\n",
    "    X -- data, numpy array of shape (input size, number of examples)\n",
    "    parameters -- output of initialize_parameters_deep()\n",
    "    \n",
    "    Returns:\n",
    "    AL -- last post-activation value\n",
    "    caches -- list of caches containing:\n",
    "                every cache of linear_activation_forward() (there are L-1 of them, indexed from 0 to L-1)\n",
    "    \"\"\"\n",
    "\n",
    "    caches = []\n",
    "    A = X\n",
    "    L = len(parameters) // 2    # number of layers in the neural network\n",
    "    \n",
    "    # Implement [LINEAR -> RELU]*(L-1). Add \"cache\" to the \"caches\" list.\n",
    "    for l in range(1, L):\n",
    "        A_prev = A \n",
    "        ### START CODE HERE ### (≈ 2 lines of code)\n",
    "        W = parameters[\"W\" + str(l)]\n",
    "        b = parameters[\"b\" + str(l)]\n",
    "        A, cache = linear_activation_forward(A_prev, W, b, activation=\"relu\")\n",
    "        caches.append(cache)\n",
    "        ### END CODE HERE ###\n",
    "    \n",
    "    # Implement LINEAR -> SIGMOID. Add \"cache\" to the \"caches\" list.\n",
    "    ### START CODE HERE ### (≈ 2 lines of code)\n",
    "    AL, cache = linear_activation_forward(A, parameters[\"W\" + str(L)], parameters[\"b\" + str(L)], activation=\"sigmoid\")\n",
    "    caches.append(cache)\n",
    "    ### END CODE HERE ###\n",
    "    \n",
    "    assert(AL.shape == (1,X.shape[1]))\n",
    "            \n",
    "    return AL, caches"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a1695fc",
   "metadata": {},
   "source": [
    "**Verify that code works as expected**\n",
    "\n",
    "Compare output of your implemented function with expected output below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "10909ee6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AL = [[0.49999827]]\n",
      "Length of caches list = 3\n"
     ]
    }
   ],
   "source": [
    "example_layer_dims = [4,5,5,1]\n",
    "\n",
    "np.random.seed(RANDOM_STATE)\n",
    "example_batch_size = 1  # number of data samples in this batch\n",
    "\n",
    "A_prev = np.random.randn(example_layer_dims[0], example_batch_size)\n",
    "W = np.random.randn(example_layer_dims[1], example_layer_dims[0])\n",
    "b = np.random.randn(example_layer_dims[1], 1)\n",
    "\n",
    "X = A_prev\n",
    "parameters = initialize_parameters(example_layer_dims)\n",
    "\n",
    "\n",
    "AL, caches = L_model_forward(X, parameters)\n",
    "print(\"AL = \" + str(AL))\n",
    "print(\"Length of caches list = \" + str(len(caches)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "351b1975",
   "metadata": {},
   "source": [
    "#### Expected output:\n",
    "\n",
    "AL = [[0.49999827]]  \n",
    "Length of caches list = 3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d445256a",
   "metadata": {},
   "source": [
    "## 4) Loss function\n",
    "\n",
    "So far we have implemented an L-layer neural network with full forward propagation, that takes input X (matrix of samples) and outputs a row vector containing the network's predictions. We also store all intermediate values that are calculated during forward propagation in \"caches\". Output of the neural net is given in $A^{[L]}$, which we can use to compute the loss of the net's predictions.\n",
    "\n",
    "We will now implement the loss calculation. In this exercise, we use binary cross entropy loss, which is given as:\n",
    "\n",
    "$$-\\frac{1}{m} \\sum\\limits_{i = 1}^{m} (y^{(i)}\\ln\\left(a^{[L] (i)}\\right) + (1-y^{(i)})\\ln\\left(1- a^{[L](i)}\\right))$$\n",
    "\n",
    "with  \n",
    "m = number of samples  \n",
    "$a^{[L]}$ = output of the model   \n",
    "y = ground truth class (0 or 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8864a4e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_cost(AL, Y):\n",
    "    \"\"\"\n",
    "    Implement the cost function defined by equation (7).\n",
    "\n",
    "    Arguments:\n",
    "    AL -- probability vector corresponding to your label predictions, shape (1, number of examples)\n",
    "    Y -- true \"label\" vector (for example: containing 0 if non-cat, 1 if cat), shape (1, number of examples)\n",
    "\n",
    "    Returns:\n",
    "    cost -- cross-entropy cost\n",
    "    \"\"\"\n",
    "    \n",
    "    m = Y.shape[1] # number of samples\n",
    "\n",
    "    # Compute loss from aL and y.\n",
    "    ### START CODE HERE ### (≈ 1 lines of code)\n",
    "    cost = -(1/m) * np.sum(Y * np.log(AL) + (1-Y) * np.log(1-AL))\n",
    "    ### END CODE HERE ###\n",
    "    \n",
    "    cost = np.squeeze(cost)      # To make sure your cost's shape is what we expect (e.g. this turns [[17]] into 17).\n",
    "    assert(cost.shape == ())\n",
    "    \n",
    "    return cost"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fb235ef",
   "metadata": {},
   "source": [
    "**Verify that code works as expected**\n",
    "\n",
    "Compare output of your implemented function with expected output below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "31c2c6aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cost = 0.6931437185364475\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(RANDOM_STATE)\n",
    "\n",
    "Y = np.array(np.random.randint(0,1, size=(1,1)))\n",
    "\n",
    "#Y, AL = compute_cost_test_case()\n",
    "\n",
    "print(\"cost = \" + str(compute_cost(AL, Y)))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90a6ccb4",
   "metadata": {},
   "source": [
    "#### Expected output:\n",
    "    \n",
    "cost = 0.6931437185364475"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c610199",
   "metadata": {},
   "source": [
    "## 5) Backward propagation\n",
    "\n",
    "Following the forward propagation of our network, we now need to implement helper functions for backpropagation. Remember that backpropagation is used to calculate the gradient of the loss function with respect to the parameters, so that we can update them using gradient descent and reduce the defined loss (and classification error as a consequence). These weight/bias updates represent learning of the network.\n",
    "\n",
    "Similar to forward propagation, we build the backward propagation in three steps:\n",
    "\n",
    "- LINEAR backward\n",
    "- LINEAR -> ACTIVATION backward where ACTIVATION computes the derivative of either the ReLU or sigmoid activation\n",
    "[LINEAR -> RELU]\n",
    "- (L-1) -> LINEAR -> SIGMOID backward (whole model)\n",
    "\n",
    "### 5.1) Linear backward function\n",
    "\n",
    "For layer $l$, the linear (forward) function is given as: $Z^{[l]} = W^{[l]} A^{[l-1]} + b^{[l]}$ (followed by an activation).\n",
    "\n",
    "Suppose we have already calculated the derivative $dZ^{[l]} = \\frac{\\partial \\mathcal{L} }{\\partial Z^{[l]}}$. We need to get $(dW^{[l]}, db^{[l]}, dA^{[l-1]})$ which are used by gradient descent to update the parameters $(W^{[l]}, b^{[l]})$.\n",
    "\n",
    "$dW^{[l]}, db^{[l]}, dA^{[l-1]}$ are computed using $dZ^{[l]}$ as follows:\n",
    "\n",
    "$$ dW^{[l]} = \\frac{\\partial \\mathcal{J} }{\\partial W^{[l]}} = \\frac{1}{m} dZ^{[l]} A^{[l-1] T}$$\n",
    "$$ db^{[l]} = \\frac{\\partial \\mathcal{J} }{\\partial b^{[l]}} = \\frac{1}{m} \\sum_{i = 1}^{m} dZ^{[l](i)}$$\n",
    "$$ dA^{[l-1]} = \\frac{\\partial \\mathcal{L} }{\\partial A^{[l-1]}} = W^{[l] T} dZ^{[l]}$$\n",
    "\n",
    "Use these formulas as reference to implement the function below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "d8fbd766",
   "metadata": {},
   "outputs": [],
   "source": [
    "def linear_backward(dZ, cache):\n",
    "    # Here cache is \"linear_cache\" containing (A_prev, W, b) coming from the forward propagation in the current layer\n",
    "    \"\"\"\n",
    "    Implement the linear portion of backward propagation for a single layer (layer l)\n",
    "\n",
    "    Arguments:\n",
    "    dZ -- Gradient of the cost with respect to the linear output (of current layer l)\n",
    "    cache -- tuple of values (A_prev, W, b) coming from the forward propagation in the current layer\n",
    "\n",
    "    Returns:\n",
    "    dA_prev -- Gradient of the cost with respect to the activation (of the previous layer l-1), same shape as A_prev\n",
    "    dW -- Gradient of the cost with respect to W (current layer l), same shape as W\n",
    "    db -- Gradient of the cost with respect to b (current layer l), same shape as b\n",
    "    \"\"\"\n",
    "    A_prev, W, b = cache\n",
    "    m = A_prev.shape[1]\n",
    "\n",
    "    ### START CODE HERE ### (≈ 3 lines of code); einfach von den Formeln übertragen\n",
    "    dW = (1/m) * np.dot(dZ, A_prev.T) # .T ist die operation für Matrixtransposition\n",
    "    db = (1/m) * np.sum(dZ, axis=1, keepdims=True)\n",
    "    dA_prev = np.dot(W.T, dZ) # wieder matrix transposition\n",
    "    ### END CODE HERE ###\n",
    "    \n",
    "    assert (dA_prev.shape == A_prev.shape)\n",
    "    assert (dW.shape == W.shape)\n",
    "    assert (db.shape == b.shape)\n",
    "    \n",
    "    return dA_prev, dW, db"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ac82f14",
   "metadata": {},
   "source": [
    "**Verify that code works as expected**\n",
    "\n",
    "Compare output of your implemented function with expected output below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "c5a1f055",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dA_prev = [[-0.00167838]\n",
      " [ 0.00305669]\n",
      " [ 0.00023985]\n",
      " [-0.00414566]\n",
      " [ 0.00043855]]\n",
      "dW = [[0.         0.         0.         0.00041755 0.        ]]\n",
      "db = [[0.49999827]]\n"
     ]
    }
   ],
   "source": [
    "example_layer_dims = [4,10,10,1]\n",
    "\n",
    "np.random.seed(RANDOM_STATE)\n",
    "\n",
    "#linear_cache = (A_prev, W, b)\n",
    "linear_cache = caches[-1][0]\n",
    "\n",
    "#Z, _ = linear_forward(A_prev, W, b)\n",
    "Z, _ = linear_forward(linear_cache[0], linear_cache[1], linear_cache[2])\n",
    "\n",
    "dAL = - (np.divide(Y, AL) - np.divide(1 - Y, 1 - AL))\n",
    "dZ = sigmoid_backward(dAL, Z)\n",
    "\n",
    "dA_prev, dW, db = linear_backward(dZ, linear_cache)\n",
    "print (\"dA_prev = \"+ str(dA_prev))\n",
    "print (\"dW = \" + str(dW))\n",
    "print (\"db = \" + str(db))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae48710f",
   "metadata": {},
   "source": [
    "#### Expected output:\n",
    "    \n",
    "dA_prev = [[-0.00167838]\n",
    " [ 0.00305669]\n",
    " [ 0.00023985]\n",
    " [-0.00414566]\n",
    " [ 0.00043855]]\n",
    " \n",
    "dW = [[0.         0.         0.         0.00041755 0.        ]]\n",
    "\n",
    "db = [[0.49999827]]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e6f1ef9",
   "metadata": {},
   "source": [
    "### 5.2) Linear activation backward function\n",
    "\n",
    "Next, we'll implement a function that merges the backward step in linear_backward and the backward step for the activation function, i.e. we implement the backpropgation for a layer consisting of LINEAR->ACTIVATION. Note that we have already implemented the backward step (i.e. calculation of the derivative) for the activation functions sigmoid and ReLU with consideration of the chain rule used in backpropagation further above (called sigmoid_backward and relu_backward). \n",
    "Mathematically, what happens in those functions is that if $g(.)$ is the activation function, then \n",
    "    `sigmoid_backward` and `relu_backward` compute $$dZ^{[l]} = dA^{[l]} * g'(Z^{[l]})$$\n",
    "    \n",
    "Use sigmoid_backward, relu_backward and linear_backward to complete the function below.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "ed0e45ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "def linear_activation_backward(dA, cache, activation):\n",
    "    \"\"\"\n",
    "    Implement the backward propagation for the LINEAR->ACTIVATION layer.\n",
    "    \n",
    "    Arguments:\n",
    "    dA -- post-activation gradient for current layer l \n",
    "    cache -- tuple of values (linear_cache, activation_cache) we store for computing backward propagation efficiently\n",
    "    activation -- the activation to be used in this layer, stored as a text string: \"sigmoid\" or \"relu\"\n",
    "    \n",
    "    Returns:\n",
    "    dA_prev -- Gradient of the cost with respect to the activation (of the previous layer l-1), same shape as A_prev\n",
    "    dW -- Gradient of the cost with respect to W (current layer l), same shape as W\n",
    "    db -- Gradient of the cost with respect to b (current layer l), same shape as b\n",
    "    \"\"\"\n",
    "    linear_cache, activation_cache = cache\n",
    "    \n",
    "    if activation == \"relu\":\n",
    "        ### START CODE HERE ### (≈ 1 line of code)\n",
    "        dZ = relu_backward(dA, activation_cache)\n",
    "        ### END CODE HERE ###\n",
    "        \n",
    "    elif activation == \"sigmoid\":\n",
    "        ### START CODE HERE ### (≈ 1 line of code)\n",
    "        dZ = sigmoid_backward(dA, activation_cache)\n",
    "        ### END CODE HERE ###\n",
    "    \n",
    "    ### START CODE HERE ### (≈ 1 line of code)\n",
    "    dA_prev, dW, db = linear_backward(dZ, linear_cache)\n",
    "    ### END CODE HERE ###\n",
    "    \n",
    "    return dA_prev, dW, db"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bdf399d",
   "metadata": {},
   "source": [
    "**Verify that code works as expected**\n",
    "\n",
    "Compare output of your implemented function with expected output below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "7e6154bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sigmoid:\n",
      "dA_prev = [[-0.00167838]\n",
      " [ 0.00305669]\n",
      " [ 0.00023985]\n",
      " [-0.00414566]\n",
      " [ 0.00043855]]\n",
      "dW = [[0.         0.         0.         0.00041755 0.        ]]\n",
      "db = [[0.49999827]]\n",
      "\n",
      "relu:\n",
      "dA_prev = [[ 1.35963386e-02]\n",
      " [ 5.01857207e-03]\n",
      " [-8.44213704e-03]\n",
      " [ 9.76147160e-08]\n",
      " [ 5.42352572e-03]]\n",
      "dW = [[0.         0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.         0.        ]\n",
      " [0.07430675 0.         0.03073885 0.         0.01554262]\n",
      " [0.         0.         0.         0.         0.        ]]\n",
      "db = [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]]\n"
     ]
    }
   ],
   "source": [
    "linear_activation_cache = caches[-1]\n",
    "\n",
    "\n",
    "dA_prev, dW, db = linear_activation_backward(dAL, linear_activation_cache, activation = \"sigmoid\")\n",
    "print (\"sigmoid:\")\n",
    "print (\"dA_prev = \"+ str(dA_prev))\n",
    "print (\"dW = \" + str(dW))\n",
    "print (\"db = \" + str(db) + \"\\n\")\n",
    "\n",
    "linear_activation_cache = caches[-2]\n",
    "\n",
    "dA_prev, dW, db = linear_activation_backward(dA_prev, linear_activation_cache, activation = \"relu\")\n",
    "print (\"relu:\")\n",
    "print (\"dA_prev = \"+ str(dA_prev))\n",
    "print (\"dW = \" + str(dW))\n",
    "print (\"db = \" + str(db))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef39261c",
   "metadata": {},
   "source": [
    "#### Expected output:\n",
    "\n",
    "\n",
    "##### sigmoid:\n",
    "\n",
    "sigmoid:\n",
    "dA_prev = [[-0.00167838]\n",
    " [ 0.00305669]\n",
    " [ 0.00023985]\n",
    " [-0.00414566]\n",
    " [ 0.00043855]]\n",
    " \n",
    "dW = [[0.         0.         0.         0.00041755 0.        ]]\n",
    "\n",
    "db = [[0.49999827]]\n",
    "\n",
    "\n",
    "##### relu:\n",
    "\n",
    "dA_prev = [[ 1.35963386e-02]\n",
    " [ 5.01857207e-03]\n",
    " [-8.44213704e-03]\n",
    " [ 9.76147160e-08]\n",
    " [ 5.42352572e-03]]\n",
    " \n",
    "dW = [[0.         0.         0.         0.         0.        ]\n",
    " [0.         0.         0.         0.         0.        ]\n",
    " [0.         0.         0.         0.         0.        ]\n",
    " [0.07430675 0.         0.03073885 0.         0.01554262]\n",
    " [0.         0.         0.         0.         0.        ]]\n",
    "\n",
    "db = [[0.]\n",
    " [0.]\n",
    " [0.]\n",
    " [1.]\n",
    " [0.]]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68f30a83",
   "metadata": {},
   "source": [
    "### 5.3) L-Layer Model Backward\n",
    "\n",
    "We can now implement the loss backpropagation for the whole network. Recall that when we run the L_model_forward function, in each step we store a cache which contains (A,W,b, and Z). In the backpropagation module, we'll use those variables to compute the gradients. Therefore, in the L_model_backward function, we go through all the hidden layers backward, starting from layer L. In each step, we use the cached values for the l-th layer to backpropagate the error through that layer.\n",
    "\n",
    "**Initializing backpropagation:** To backpropagate the loss through our network with final sigmoid activation in the output layer, we know that the output is given as: $A^{[L]} = \\sigma(Z^{[L]})$\n",
    ". Our code thus needs to compute $dAL= \\frac{\\partial \\mathcal{L}}{\\partial A^{[L]}}$, where $\\mathcal{L}$ is our loss function.\n",
    "\n",
    "Thus, we need the partial derivative of our loss function with respect to AL, which is given by the following formula:\n",
    "\n",
    "$dAL = - (np.divide(Y, AL) - np.divide(1 - Y, 1 - AL))$\n",
    "\n",
    "We can use this post-activation gradient dAL to keep going backward by feeding dAL into our LINEAR->SIGMOID backward function (which will use the cached values stored by the L_model_forward function). After that, we'll have to use a for loop to iterate through all the remaining layers using our LINEAR->RELU backward function. As output of the function, all calculated dA, dW, and db (i.e. for each layer) has to be stored in a python dict (called \"grads\", see below) using the naming conventions outlined below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "bbb114c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def L_model_backward(AL, Y, caches):\n",
    "    \"\"\"\n",
    "    Implement the backward propagation for the [LINEAR->RELU] * (L-1) -> LINEAR -> SIGMOID group\n",
    "    \n",
    "    Arguments:\n",
    "    AL -- probability vector, output of the forward propagation (L_model_forward())\n",
    "    Y -- true \"label\" vector (containing 0 if non-cat, 1 if cat)\n",
    "    caches -- list of caches containing:\n",
    "                every cache of linear_activation_forward() with \"relu\" (it's caches[l], for l in range(L-1) i.e l = 0...L-2)\n",
    "                the cache of linear_activation_forward() with \"sigmoid\" (it's caches[L-1])\n",
    "    \n",
    "    Returns:\n",
    "    grads -- A dictionary with the gradients\n",
    "             grads[\"dA\" + str(l)] = ... \n",
    "             grads[\"dW\" + str(l)] = ...\n",
    "             grads[\"db\" + str(l)] = ... \n",
    "    \"\"\"\n",
    "    grads = {}\n",
    "    L = len(caches) # the number of layers\n",
    "    m = AL.shape[1]\n",
    "    Y = Y.reshape(AL.shape) # after this line, Y is the same shape as AL\n",
    "    \n",
    "    # Initializing the backpropagation\n",
    "    ### START CODE HERE ### (1 line of code)\n",
    "    dAL = -(np.divide(Y,AL) - np.divide(1-Y, 1-AL))\n",
    "    ### END CODE HERE ###\n",
    "    \n",
    "    # Lth layer (SIGMOID -> LINEAR) gradients. Inputs: \"dAL, current_cache\". Outputs: \"grads[\"dAL-1\"], grads[\"dWL\"], grads[\"dbL\"]\n",
    "    ### START CODE HERE ### (approx. 2 lines)\n",
    "    current_cache = caches[L-1]\n",
    "    grads[\"dA\" + str(L-1)], grads[\"dW\" + str(L)], grads[\"db\" + str(L)] = linear_activation_backward(dAL, current_cache, activation=\"sigmoid\")\n",
    "    ### END CODE HERE ###\n",
    "    \n",
    "    # Loop from l=L-2 to l=0\n",
    "    for l in reversed(range(L-1)):\n",
    "        # lth layer: (RELU -> LINEAR) gradients.\n",
    "        # Inputs: \"grads[\"dA\" + str(l + 1)], current_cache\". Outputs: \"grads[\"dA\" + str(l)] , grads[\"dW\" + str(l + 1)] , grads[\"db\" + str(l + 1)] \n",
    "        ### START CODE HERE ### (approx. 5 lines)\n",
    "        current_cache = caches[l]\n",
    "        dA_prev_temp, dW_temp, db_temp = linear_activation_backward(grads[\"dA\" + str(l+1)], current_cache, activation=\"relu\")\n",
    "        grads[\"dA\" + str(l)] = dA_prev_temp\n",
    "        grads[\"dW\" + str(l+1)] = dW_temp\n",
    "        grads[\"db\" + str(l+1)] = db_temp\n",
    "        ### END CODE HERE ###\n",
    "\n",
    "    return grads\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f43592a8",
   "metadata": {},
   "source": [
    "**Verify that code works as expected**\n",
    "\n",
    "Compare output of your implemented function with expected output below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "3ceb6892",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dW1 = [[-0.41675785 -0.05626683 -2.1361961   1.64027081]\n",
      " [ 0.          0.          0.          0.        ]\n",
      " [-0.41675785 -0.05626683 -2.1361961   1.64027081]\n",
      " [ 0.          0.          0.          0.        ]\n",
      " [-0.41675785 -0.05626683 -2.1361961   1.64027081]]\n",
      "db1 = [[1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]]\n",
      "dA1 = [[ 1.35963386e-02]\n",
      " [ 5.01857207e-03]\n",
      " [-8.44213704e-03]\n",
      " [ 9.76147160e-08]\n",
      " [ 5.42352572e-03]]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "grads = L_model_backward(AL, Y, caches)\n",
    "print(\"dW1 = {}\\ndb1 = {}\\ndA1 = {}\\n\".format(grads['dW1'], grads['db1'], grads['dA1']))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3144c392",
   "metadata": {},
   "source": [
    "#### Expected output:\n",
    "\n",
    "dW1 = [[-0.41675785 -0.05626683 -2.1361961   1.64027081]\n",
    " [ 0.          0.          0.          0.        ]\n",
    " [-0.41675785 -0.05626683 -2.1361961   1.64027081]\n",
    " [ 0.          0.          0.          0.        ]\n",
    " [-0.41675785 -0.05626683 -2.1361961   1.64027081]]\n",
    " \n",
    "db1 = [[1.]\n",
    " [0.]\n",
    " [1.]\n",
    " [0.]\n",
    " [1.]]\n",
    " \n",
    "dA1 = [[ 1.35963386e-02]\n",
    " [ 5.01857207e-03]\n",
    " [-8.44213704e-03]\n",
    " [ 9.76147160e-08]\n",
    " [ 5.42352572e-03]]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2276310",
   "metadata": {},
   "source": [
    "### 5.4) Update parameters\n",
    "\n",
    "We now have everything to update the parameters (weight matrices and bias vectors) of the model using gradient descent. These updates are essentially what constitutes \"learning\" in the neural network. We update weights and biases as follows:\n",
    "\n",
    "$$ W^{[l]} = W^{[l]} - \\alpha \\text{ } dW^{[l]} $$\n",
    "$$ b^{[l]} = b^{[l]} - \\alpha \\text{ } db^{[l]} $$\n",
    "\n",
    "where $\\alpha$ is the learning rate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "0f555dc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_parameters(parameters, grads, learning_rate):\n",
    "    \"\"\"\n",
    "    Update parameters using gradient descent\n",
    "    \n",
    "    Arguments:\n",
    "    parameters -- python dictionary containing your parameters \n",
    "    grads -- python dictionary containing your gradients, output of L_model_backward\n",
    "    \n",
    "    Returns:\n",
    "    parameters -- python dictionary containing your updated parameters \n",
    "                  parameters[\"W\" + str(l)] = ... \n",
    "                  parameters[\"b\" + str(l)] = ...\n",
    "    \"\"\"\n",
    "    \n",
    "    L = len(parameters) // 2  # number of layers in the neural network\n",
    "\n",
    "    # Update rule for each parameter. Use a for loop.\n",
    "    ### START CODE HERE ### (≈ 3 lines of code)\n",
    "    for l in range (1, L+1):\n",
    "        parameters[\"W\" + str(l)] -= learning_rate * grads[\"dW\" + str(l)]\n",
    "        parameters[\"b\" + str(l)] -= learning_rate * grads[\"db\" + str(l)]\n",
    "\n",
    "    ### END CODE HERE ###\n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa739362",
   "metadata": {},
   "source": [
    "**Verify that code works as expected**\n",
    "\n",
    "Compare output of your implemented function with expected output below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "86743873",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "W1 = [[ 0.03750821  0.00506401  0.19225765 -0.14762437]\n",
      " [-0.01793436 -0.00841747  0.00502881 -0.01245288]\n",
      " [ 0.03109626 -0.00346339  0.21913415 -0.141105  ]\n",
      " [ 0.00041539 -0.01117925  0.00539058 -0.0059616 ]\n",
      " [ 0.04148448  0.01737669  0.2061409  -0.16393683]]\n",
      "b1 = [[-0.1]\n",
      " [ 0. ]\n",
      " [-0.1]\n",
      " [ 0. ]\n",
      " [-0.1]]\n",
      "W2 = [[-8.78107893e-03 -1.56434170e-03  2.56570452e-03 -9.88779049e-03\n",
      "  -3.38821966e-03]\n",
      " [-2.36184031e-03 -6.37655012e-03 -1.18761229e-02 -1.42121723e-02\n",
      "  -1.53495196e-03]\n",
      " [-2.69056960e-03  2.23136679e-02 -2.43476758e-02  1.12726505e-03\n",
      "   3.70444537e-03]\n",
      " [ 6.16566348e-03  5.01857207e-03 -1.15160218e-02  9.76147160e-08\n",
      "   3.86926367e-03]\n",
      " [-3.13508197e-03  7.71011738e-03 -1.86809065e-02  1.73118467e-02\n",
      "   1.46767801e-02]]\n",
      "b2 = [[ 0. ]\n",
      " [ 0. ]\n",
      " [ 0. ]\n",
      " [-0.1]\n",
      " [ 0. ]]\n",
      "W3 = [[-0.00335677  0.00611341  0.00047971 -0.00833311  0.0008771 ]]\n",
      "b3 = [[-0.04999983]]\n"
     ]
    }
   ],
   "source": [
    "parameters = update_parameters(parameters, grads, 0.1)\n",
    "\n",
    "print (\"W1 = \"+ str(parameters[\"W1\"]))\n",
    "print (\"b1 = \"+ str(parameters[\"b1\"]))\n",
    "print (\"W2 = \"+ str(parameters[\"W2\"]))\n",
    "print (\"b2 = \"+ str(parameters[\"b2\"]))\n",
    "print (\"W3 = \"+ str(parameters[\"W3\"]))\n",
    "print (\"b3 = \"+ str(parameters[\"b3\"]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e50fd738",
   "metadata": {},
   "source": [
    "#### Expected output:\n",
    "\n",
    "W1 = [[ 0.03750821  0.00506401  0.19225765 -0.14762437]\n",
    " [-0.01793436 -0.00841747  0.00502881 -0.01245288]\n",
    " [ 0.03109626 -0.00346339  0.21913415 -0.141105  ]\n",
    " [ 0.00041539 -0.01117925  0.00539058 -0.0059616 ]\n",
    " [ 0.04148448  0.01737669  0.2061409  -0.16393683]]\n",
    " \n",
    "b1 = [[-0.1]\n",
    " [ 0. ]\n",
    " [-0.1]\n",
    " [ 0. ]\n",
    " [-0.1]]\n",
    " \n",
    "W2 = [[-8.78107893e-03 -1.56434170e-03  2.56570452e-03 -9.88779049e-03\n",
    "  -3.38821966e-03]\n",
    " [-2.36184031e-03 -6.37655012e-03 -1.18761229e-02 -1.42121723e-02\n",
    "  -1.53495196e-03]\n",
    " [-2.69056960e-03  2.23136679e-02 -2.43476758e-02  1.12726505e-03\n",
    "   3.70444537e-03]\n",
    " [ 6.16566348e-03  5.01857207e-03 -1.15160218e-02  9.76147160e-08\n",
    "   3.86926367e-03]\n",
    " [-3.13508197e-03  7.71011738e-03 -1.86809065e-02  1.73118467e-02\n",
    "   1.46767801e-02]]\n",
    "   \n",
    "b2 = [[ 0. ]\n",
    " [ 0. ]\n",
    " [ 0. ]\n",
    " [-0.1]\n",
    " [ 0. ]]\n",
    " \n",
    "W3 = [[-0.00335677  0.00611341  0.00047971 -0.00833311  0.0008771 ]]\n",
    "\n",
    "b3 = [[-0.04999983]]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b93e842",
   "metadata": {},
   "source": [
    "## 6) Optional\n",
    "\n",
    "We now have implemented all functions that are needed to build and train a deep neural network for a binary classification task. Try putting it all together and train a DNN for the last binary classification task on Exercise sheet 3 (data given in \"ex3data2.mat\").   \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94e7b20c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "b15cdbd9",
   "metadata": {},
   "source": [
    "# 7) Q&A (Bonus Points)      Due: Fr, 15.07.22, 23:59\n",
    "\n",
    "#### (T7) Your task: Answer the questions below and give explanations where required. \n",
    "\n",
    "- 7.1 ) Describe the components of a Multi Layer Perceptron.\\\n",
    "Input Layer - receives input data and passes it to the next layer. Number of neurons in input layer -> number of input features in the dataset.\\\n",
    "Hidden Layer - stand between input and output layer. they are responsible for learning the patterns within the data.\\\n",
    "Neuron - receives input, applies an activation function to get a value. The output is determined by the weighted sum of its inputs, passed through the activation function (only if they are in hidden layers or the output layer).\\\n",
    "Output Layer - produces the final prediction of the network. The number of neurons in the layer depends on the task of the network. For example, for binary classification, there might only be a single neuron.\\\n",
    "Weights and Biases - they form the connection between neurons. Each neuron has its own weights, which determine the strength of the connection. Also, each neuron has a bias value, which can shift the threshold of the activation function.\n",
    "\n",
    "- 7.2 ) What do Neural Networks need to have to be able to estimate non-linear relationships in the data?\\\n",
    "Neural networks definitely need to have Activation Functions and a sizable training dataset to estimate non-linear relationships.\n",
    "\n",
    "- 7.3 ) For a binary classification task on 5 input features; Imagine a deep Neural Network that consists of 3 hidden, fully connected layers with 1000, 500 and 1000 nodes respectively. How many total weights does the network have?\\\n",
    "5000 +  1000 * 500  +  500 × 1000  +  1000  =  1 006 000 weights\n",
    "\n",
    "- 7.4 ) Describe a problem that large and very deep Neural Network architectures can have for small data sets.\\\n",
    "When dealing with a small dataset, a large and very deep NN has a higher risk of overfitting.\n",
    "\n",
    "- 7.5 ) Describe how the prediction error for the training samples is used to estimate the weights of a deep Neural Network. Use the words \"Loss function\", \"Back Propagation\" and \"Gradient Descent\"\\\n",
    "The loss function measures the difference between the network's predictions and the ground truth of the training samples. Through backpropagation, the gradient of the loss function with respect to the network's weights is calculated. This helps us to determine how much each weight affects the overall cost. Then, we want to find the weights that minimize the loss function and reduce the cost. By using a technique called Gradient Descent, the weights get adjusted with the inverted gradient, and the NN iteratively minimizes the cost.  "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
