{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "43cf7572",
   "metadata": {},
   "source": [
    "# Exercise sheet 3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f9eb824",
   "metadata": {},
   "source": [
    "In this third exercise sheet we will work with classification algorithms. We'll start again by loading some of the needed libraries.\n",
    "\n",
    "Note that comments in the coding exercise cells below just serve as hints and are not requirements for your own code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e4f6850b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plt to plot figures\n",
    "import matplotlib.pyplot as plt\n",
    "# numpy for array/matrix operations\n",
    "import numpy as np\n",
    "# to be able to load matlab data files\n",
    "from scipy.io import loadmat\n",
    "\n",
    "# we set a seed variable for functions that use randomization (e.g. when shuffling data samples)\n",
    "# this way, we can have reproducible results even with randomization\n",
    "RANDOM_STATE = 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "291b8a62",
   "metadata": {},
   "source": [
    "## 1) Support Vector Machines\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb37f3ef",
   "metadata": {},
   "source": [
    "### 1.1) Linearly separable datasets\n",
    "\n",
    "The first thing we're going to do is look at a simple 2-dimensional data set and see how a linear SVM works on this data. Let's load the data and visualize it as a scatter plot where the class label is denoted by a symbol ('+' for positive, 'o' for negative).\n",
    "\n",
    "\n",
    "#### (T1.1) Your Task: Use PCA to transform your data and project it to a 2-dimensional space. Visualize the result as a scatter plot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7ac95498",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (785635987.py, line 3)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  Input \u001b[1;32mIn [2]\u001b[1;36m\u001b[0m\n\u001b[1;33m    X, y =\u001b[0m\n\u001b[1;37m           ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "raw_data = loadmat('ex3data1.mat')\n",
    "# YOUR CODE HERE\n",
    "X, y = \n",
    "\n",
    "fig, ax = plt.subplots(figsize=(12,8))\n",
    "ax.scatter(X[np.where(y==1)[0],0],X[np.where(y==1)[0],1], s=50, marker='x', label='Positive')\n",
    "ax.scatter(X[np.where(y==0)[0],0],X[np.where(y==0)[0],1], s=50, marker='o', label='Negative')\n",
    "\n",
    "ax.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "229897f8",
   "metadata": {},
   "source": [
    "#### Observation\n",
    "\n",
    "Notice that there is one outlier positive example that sits apart from the others. The classes are still linearly separable but it's a very tight fit. We're going to train a linear support vector machine to learn the class boundary.\n",
    "\n",
    "#### (T1.2) Your Task: Train and evaluate a linear SVM for this dataset (train and evaluate on the same data; use z-normalization)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cdbcd01",
   "metadata": {},
   "outputs": [],
   "source": [
    "RANDOM_STATE = 2\n",
    "np.random.seed(RANDOM_STATE)\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# load linear SVM algorithm\n",
    "from sklearn.svm import LinearSVC\n",
    "\n",
    "\n",
    "# YOUR CODE HERE\n",
    "# z-normalization of the data\n",
    "\n",
    "\n",
    "# train and classify with LinearSVC\n",
    "\n",
    "\n",
    "print(result)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04a5b1fb",
   "metadata": {},
   "source": [
    "### 1.2) SVM regularization parameter C\n",
    "\n",
    "The C parameter trades off correct classification of training examples against maximization of the decision function’s margin. For larger values of C, a smaller margin will be accepted if the decision function is better at classifying all training points correctly. A lower C will encourage a larger margin, therefore a simpler decision function, at the cost of training accuracy. In other words C behaves as a regularization parameter in the SVM.\n",
    "\n",
    "Let's take a look at the effects of parameter C on the decision hyperplane, support vectors and accuracy. \n",
    "\n",
    "\n",
    "#### (T1.3) Your Task: Train two linear SVM with two values for C (0.5 and 100). Plot the resulting decision boundaries, margins and the support vectors for each resulting SVM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea7636ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import cm\n",
    "# note: we need to use SVC class here because LinearSVC doesn't have certain attributes that are \n",
    "# needed for plotting\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "fignum=1\n",
    "\n",
    "for name, C_value in ((\"unregularized\", 100), (\"regularized\", 0.5)):\n",
    "\n",
    "    # instantiate and fit SVM with linear kernel and with C=C_value\n",
    "    # YOUR CODE HERE, TwO LINES\n",
    "    svm = \n",
    "\n",
    "    # get accuracy of the SVM\n",
    "    # YOUR CODE HERE\n",
    "    acc = \n",
    "    \n",
    "    # get the separating hyperplane\n",
    "    w = svm.coef_[0]\n",
    "    a = -w[0] / w[1]\n",
    "    xx = np.linspace(-5, 5)\n",
    "    yy = a * xx - (svm.intercept_[0]) / w[1]\n",
    "\n",
    "    # plot the parallels to the separating hyperplane that pass through the\n",
    "    # support vectors (margin away from hyperplane in direction\n",
    "    # perpendicular to hyperplane). This is sqrt(1+a^2) away vertically in\n",
    "    # 2-d.\n",
    "    margin = 1 / np.sqrt(np.sum(svm.coef_ ** 2))\n",
    "    yy_down = yy - np.sqrt(1 + a ** 2) * margin\n",
    "    yy_up = yy + np.sqrt(1 + a ** 2) * margin\n",
    "\n",
    "    # plot the line, the points, and the nearest vectors to the plane\n",
    "    plt.figure(fignum, figsize=(8, 6))\n",
    "    plt.clf()\n",
    "    plt.plot(xx, yy, \"k-\")\n",
    "    plt.plot(xx, yy_down, \"k--\")\n",
    "    plt.plot(xx, yy_up, \"k--\")\n",
    "\n",
    "    \n",
    "    # mark the support vectors in the plot\n",
    "    plt.scatter(\n",
    "        # YOUR CODE HERE, TWO PARAMETERS / LINES: x and y coordinates of support vectors (accessible as attributes in a trained svm, see doc)\n",
    "\n",
    "        \n",
    "        s=80,\n",
    "        facecolors=\"none\",\n",
    "        zorder=10,\n",
    "        edgecolors=\"k\",\n",
    "        cmap=cm.get_cmap(\"RdBu\"),\n",
    "    )\n",
    "    plt.scatter(\n",
    "        # YOUR CODE HERE, THREE ARGUMENTS: all x and y coordinates of data points and their color (=class)\n",
    "        \n",
    "        zorder=10, cmap=cm.get_cmap(\"RdBu\"), edgecolors=\"k\"\n",
    "    )\n",
    "\n",
    "    plt.axis(\"tight\")\n",
    "    \n",
    "    # set range of axes for your data points\n",
    "    # YOUR CODE HERE, 4 LINES\n",
    "    x_min = \n",
    "    x_max = \n",
    "    y_min = \n",
    "    y_max = \n",
    "\n",
    "    YY, XX = np.meshgrid(yy, xx)\n",
    "    xy = np.vstack([XX.ravel(), YY.ravel()]).T\n",
    "    Z = svm.decision_function(xy).reshape(XX.shape)\n",
    "\n",
    "    # Put the result into a contour plot\n",
    "    plt.contourf(XX, YY, Z, cmap=cm.get_cmap(\"RdBu\"), alpha=0.5, linestyles=[\"-\"])\n",
    "\n",
    "    plt.xlim(x_min, x_max)\n",
    "    plt.ylim(y_min, y_max)\n",
    "\n",
    "    plt.xticks(())\n",
    "    plt.yticks(())\n",
    "    \n",
    "    \n",
    "    plt.title(\"; \".join([name, \"C=\" + str(C_value), \"accuracy: \" + str(acc)]))\n",
    "    \n",
    "    fignum = fignum + 1\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f8bcbc2",
   "metadata": {},
   "source": [
    "#### Observation\n",
    "\n",
    "We should be able to see that low regularization (i.e. high C) results in perfect classification of the training data, however by increasing the value of C we've created a decision boundary that is not necessarily a natural fit for the data. This will become apparent when classifying new data points (but we won't do that here now)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24bf7e61",
   "metadata": {},
   "source": [
    "### 1.3) Non-linearly separable datasets\n",
    "\n",
    "In practice, we will often find that our dataset is not linearly separable and a linear classifier will not be able to perform well on it. Let's take a look at the dataset below. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a54809b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_circles\n",
    "#from mpl_toolkits.mplot3d import Axes3D\n",
    "\n",
    "# generating data\n",
    "X, y = make_circles(n_samples = 500, noise = 0.02, random_state=RANDOM_STATE)\n",
    "  \n",
    "# visualizing data\n",
    "plt.scatter(X[:, 0], X[:, 1], c = y, marker = '.')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8713b43a",
   "metadata": {},
   "source": [
    "#### Observation\n",
    "\n",
    "This dataset is clearly a non-linear dataset (classes cannot be separated by a straight line) and consists of two features (say, A and B). A linear classifier cannot separate the two classes in this dataset as is. To get further evidence for this conclusion (which in practice is rarely as obvious as in this 2-D case), let's first try our linear SVM on this dataset.\n",
    "\n",
    "#### (T1.4) Your Task: Run training and classification using a *linear* SVM classifier (use a 80/20% train/test split). Hint: Feel free to copy your code from above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d95e0aa1",
   "metadata": {},
   "outputs": [],
   "source": [
    "RANDOM_STATE = 2\n",
    "np.random.seed(RANDOM_STATE)\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# load linear SVM algorithm\n",
    "from sklearn.svm import LinearSVC\n",
    "\n",
    "# YOUR CODE HERE\n",
    "# create train/test sets\n",
    "\n",
    "\n",
    "# z-normalization\n",
    "\n",
    "\n",
    "# train and classify with LinearSVC\n",
    "\n",
    "\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f379aa85",
   "metadata": {},
   "source": [
    "#### Observation\n",
    "\n",
    "Results above should show that a linear SVM doesn't work much better than chance level on this dataset, further solidifying our initial impression, that the dataset is not linearly separable. So, we conclude now that a linear SVM, which is a linear classifier, cannot separate the two classes in this dataset. But, we can use SVM's Kernel trick to project this dataset into a higher dimensionality where the classes become separable. We'll use the popular Gaussian Kernel (\"RBF - radial basis function\") for this.\n",
    "\n",
    "Let's first recap what an RBF is by implementing the function from scratch based on the formula from the lecture:\n",
    "\n",
    "Gaussian Kernel $k(x,y) = exp(\\frac{-||x-y||^2}{2\\sigma^2})$\n",
    "\n",
    "#### (T1.5) Your Task: Implement a gaussian kernel function, also called the \"radial basis function\" (formula e.g. in the lecture). You can validate it with the given parameters/result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48849ac7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gaussian_kernel(x, y, sigma):\n",
    "    # YOUR CODE HERE\n",
    "    \n",
    "    return result\n",
    "\n",
    "\n",
    "#### code for verifying gaussian_kernel function below, no need to change\n",
    "# sample parameter values\n",
    "x1 = np.array([1.0, 2.0, 1.0])\n",
    "x2 = np.array([0.0, 4.0, -1.0])\n",
    "sigma = 2\n",
    "\n",
    "result = gaussian_kernel(x1, x2, sigma)\n",
    "expected_result = 0.32465246735834974\n",
    "if expected_result == result: \n",
    "    print(\"Correct!\")\n",
    "else:\n",
    "    print(\"Somthing went wrong.\")\n",
    "    print(\"Your result: \", result)\n",
    "    print(\"Expected result: \", expected_result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85450011",
   "metadata": {},
   "source": [
    "### 1.4) SVM with RBF Kernel\n",
    "\n",
    "Now that we gained some insight about what the RBF looks like, let's train an SVM with RBF Kernel for our dataset. We can use the already implemented RBF in sklearn for this.\n",
    "\n",
    "#### (T1.6) Your Task: Run training and classification using an SVM classifier with RBF kernel. Confirm that accuracy is now much better.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25e6c7c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load SVM algorithm\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "# YOUR CODE HERE\n",
    "# train and classify with SVC with RBF kernel\n",
    "\n",
    "\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ade3ae5",
   "metadata": {},
   "source": [
    "### 1.5) Parameter Tuning of an SVM with RBF Kernel\n",
    "\n",
    "Note that the RBF is often implemented as:\n",
    "\n",
    "$k(x,y) = exp(-\\gamma||x-y||^2)$\n",
    "\n",
    "with $\\gamma = \\frac{1}{2\\sigma^2}$\n",
    "\n",
    "Here, $\\gamma$ (gamma) becomes a tuning parameter in SVM training. When training an SVM with the Radial Basis Function (RBF) kernel (which is usually used for non-linearly separable datasets), two parameters must be considered: C (already discussed above) and gamma. \n",
    "\n",
    "Intuitively, the gamma parameter defines how far the influence of a single training example reaches, with low values meaning ‘far’ and high values meaning ‘close’. The gamma parameters can be seen as the inverse of the radius of influence of samples selected by the model as support vectors.\n",
    "\n",
    "Let's do this tuning of both C and gamma now by running a logarithmic grid search over various combinations. The best parameter combination is to be determined based on the performance on a *validation* set.\n",
    "\n",
    "*Note on validation set:* \n",
    "A validation dataset is a subset of data held back from training your model that is used to give an estimate of model skill while tuning a model's hyperparameters (like C and gamme in our case here).\n",
    "The validation dataset is different from the test dataset which is also held back from the training of the model, but is instead used to give an unbiased estimate of the skill of the final tuned model.\n",
    "\n",
    "*Note on (logarithmic) grid search:*\n",
    "A grid search (here in the context of tuning a classifier) is a search over various combinations of hyper-parameter ranges. Often hyper-parameter values can be chosen that are spaced apart based on a logarithmic scale (i.e. 0.1, 1, 10, 100, ...), which allows for a faster search over a larger parameter space. Further refinements (smaller valued steps) can then be made in the ranges that show the most promise (based on validation set performance).\n",
    "\n",
    "We'll introduce a new dataset for this task, which we first load and visualize below.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e693426",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Load new dataset\n",
    "\n",
    "# Let's load and preprocess the data first\n",
    "raw_data = loadmat('ex3data2.mat')\n",
    "\n",
    "# YOUR CODE HERE\n",
    "X_all,y_all = \n",
    "\n",
    "# visualize the data\n",
    "fig, ax = plt.subplots(figsize=(12,8))\n",
    "ax.scatter(X_all[np.where(y_all==1)[0],0],X_all[np.where(y_all==1)[0],1], s=50, marker='x', label='Positive')\n",
    "ax.scatter(X_all[np.where(y_all==0)[0],0],X_all[np.where(y_all==0)[0],1], s=50, marker='o', label='Negative')\n",
    "ax.legend()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8195f1ca",
   "metadata": {},
   "source": [
    "### Data prep\n",
    "\n",
    "As a preprocessing step for classifier training, let's create train/val/test data splits and z-normalize the data.\n",
    "\n",
    "#### (T1.7) Your Task: Create train/test set splits (80/20%). Then create train/val by taking 10% from train. Finally z-normalize all three data sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca8ceab1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for comparison/reproducibility \n",
    "RANDOM_STATE = 2\n",
    "np.random.seed(RANDOM_STATE)\n",
    "\n",
    "# YOUR CODE HERE\n",
    "# create train/val/test sets\n",
    "\n",
    "\n",
    "# z-normalization of all datasets\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cad3ca91",
   "metadata": {},
   "source": [
    "#### (T1.8) Your Task: Implement and run a logarithmic grid search over the given values for C and gamma by training an SVM (with RBF) for each (C,gamma) combination on the train set and evaluating them on the validation set. Determine the best performing (C,gamma) combination on the validation set. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41cd84c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We use logarithmic scale for the values of C/gamma to search a bigger space more quickly \n",
    "# Also, hyper-parameters might not be very sensitive to small changes\n",
    "C_values = [0.1, 0.3, 1, 3, 10, 30, 100]\n",
    "gamma_values = [0.1, 0.3, 1, 3, 10, 30, 100]\n",
    "\n",
    "# YOUR CODE HERE\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "print(\"Best validation accuracy: {}\\nBest parameters: {}\".format(best_score, best_params))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "385c93d6",
   "metadata": {},
   "source": [
    "### 1.6) Comparison with other classifiers\n",
    "\n",
    "Now we want to compare performance of various classifiers that we saw in the lecture. Let's train them and compare results. Use the dataset splits from above for this (train and test set only).\n",
    "\n",
    "#### (T1.9) Your Task: Run training and classification using the following classifiers: kNN, LDA, Decision Tree, *linear* SVM, SVM with RBF (using the best C,gamma combo from above)\n",
    "\n",
    "Based on the results, can you tell which classifier is linear and which is non-linear? Do the results match your expectations?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29a3d2a8",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "RANDOM_STATE = 2\n",
    "np.random.seed(RANDOM_STATE)\n",
    "\n",
    "\n",
    "# YOUR CODE HERE\n",
    "# kNN\n",
    "\n",
    "print(\"kNN \", acc)\n",
    "\n",
    "# LDA\n",
    "\n",
    "print(\"LDA \", acc)\n",
    "\n",
    "# linear SVM\n",
    "\n",
    "print(\"SVM (linear)\", acc)\n",
    "\n",
    "# SVM with RBF kernel and tuned C, gamma \n",
    "\n",
    "print(\"SVM (w/ RBF)\", acc)\n",
    "\n",
    "# decision tree\n",
    "\n",
    "print(\"D-Tree \", acc)\n",
    "\n",
    "\n",
    "# YOUR CODE HERE END\n",
    "\n",
    "# compare with chance level\n",
    "labels, counts = np.unique(y_test, return_counts=True)\n",
    "chance_level = max(counts) / y_test.shape[0]\n",
    "print(\"Chance: \", chance_level)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67cd66c5",
   "metadata": {},
   "source": [
    "## 2) Decision Trees\n",
    "\n",
    "Decision Trees are a non-parametric supervised learning method used for classification (and regression). The goal is to create a model that predicts the value of a target variable by learning simple decision rules inferred from the data features. A tree can be seen as a piecewise constant approximation. \n",
    "Let's take a look at the tree we created above. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b06ef2e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import tree\n",
    "plt.subplots(figsize=(20,16), dpi=100)\n",
    "print(\"Tree depth: \", dtree.get_depth())\n",
    "tree.plot_tree(dtree)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51a1c8dc",
   "metadata": {},
   "source": [
    "### Observation\n",
    "\n",
    "This tree has a depth of 10, i.e. it can take up to 10 decisions before we arrive at a leaf node. Classification paths of this tree are determined based on the feature values of the input sample that we want to classify and the rules given in each node. \n",
    "\n",
    "Let the input sample that we want to classify with this tree be x=[0.62880647 0.03242455].\n",
    "\n",
    "#### (T2.1) Your Task: a) Determine the class of this data point and b) identify the leaf node in the tree that this decision is based on. (Hint: you can do this without coding by just looking at the tree.)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "408b398e",
   "metadata": {},
   "source": [
    "# 3) Q&A (Bonus Points)      Due: Fr, 09.06.23, 23:59\n",
    "\n",
    "#### (T3) Your task: Answer the questions below and give explanations where required. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e603a95",
   "metadata": {},
   "source": [
    "1. Describe the training of a Decision Tree (Split Criterion,Termination Criterion).\n",
    "2. Name 3 good reasons for using a Decision Tree as a Machine Learning Model.\n",
    "3. Describe the data requirement and assumption for a classification with a Linear Discriminant Analysis (LDA) transform.\n",
    "4. What is the objective of the LDA transform and how do the nominator and denominator of the Fishers Ratio (J in the lecture) relate to it?\n",
    "5. Describe the optimization criterion that is used to fit the dividing hyper plane for Support Vector Machines (SVM).\n",
    "6. How is non-linear separability achieved with SVMs and why do we need the kernel trick?\n",
    "7. How are the coefficients of a regression model estimated?\n",
    "8. How is a regression model evaluated, and how can this evaluation be interpreted intuitively?\n",
    "9. Describe one method to combat overfitting for a regression model."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
